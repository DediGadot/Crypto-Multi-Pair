#!/usr/bin/env python3
"""
Comprehensive Portfolio Parameter Optimization with Walk-Forward Analysis

This script performs exhaustive parameter optimization for multi-asset portfolio
strategies using walk-forward analysis. It optimizes asset selection, weight
allocation, and rebalancing parameters to maximize outperformance vs buy-and-hold.

**Purpose**: Find optimal portfolio configuration that generalizes across
different market regimes using rigorous walk-forward validation.

**Method**:
- Walk-forward analysis with expanding training window
- Grid search across asset combinations, weights, and rebalancing params
- Out-of-sample testing to prevent overfitting
- Statistical significance testing and robustness analysis

**Third-party packages**:
- pandas: https://pandas.pydata.org/docs/
- numpy: https://numpy.org/doc/stable/
- scipy: https://docs.scipy.org/doc/scipy/
- loguru: https://loguru.readthedocs.io/en/stable/
- typer: https://typer.tiangolo.com/
- pyyaml: https://pyyaml.org/wiki/PyYAMLDocumentation

**Sample Input**:
```bash
python optimize_portfolio_comprehensive.py --window-days 365 --test-windows 5
```

**Expected Output**:
- optimized_config.yaml: Best configuration
- OPTIMIZATION_REPORT.txt: Research-grade analysis with TL;DR
- optimization_results.csv: All tested configurations
- parameter_sensitivity.csv: Parameter impact analysis

Usage:
    python optimize_portfolio_comprehensive.py
    python optimize_portfolio_comprehensive.py --window-days 365 --timeframe 1h
    python optimize_portfolio_comprehensive.py --quick-mode  # Faster testing
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Set
from itertools import product, combinations
import warnings
from collections import defaultdict

# Add src directory to Python path
script_dir = Path(__file__).resolve().parent
src_dir = script_dir / "src"
if src_dir.exists() and str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

import typer
import yaml
import pandas as pd
import numpy as np
from scipy import stats
from loguru import logger

from crypto_trader.data.fetchers import BinanceDataFetcher

# Suppress warnings
warnings.filterwarnings('ignore')

app = typer.Typer(help="Comprehensive portfolio optimization with walk-forward analysis")


class ComprehensiveOptimizer:
    """
    Research-grade portfolio optimizer with walk-forward analysis.

    Optimizes:
    1. Asset Selection: Which coins to include
    2. Weight Allocation: How to distribute capital
    3. Rebalancing Parameters: When and how to rebalance

    Uses walk-forward analysis to prevent overfitting and ensure
    out-of-sample performance.
    """

    def __init__(
        self,
        window_days: int = 365,
        timeframe: str = "1h",
        test_windows: int = 5,
        quick_mode: bool = False
    ):
        """
        Initialize the comprehensive optimizer.

        Args:
            window_days: Size of each window in days (default: 365)
            timeframe: Candle timeframe (default: "1h")
            test_windows: Number of windows for walk-forward (default: 5)
            quick_mode: If True, use smaller parameter grid for faster testing
        """
        self.window_days = window_days
        self.timeframe = timeframe
        self.test_windows = test_windows
        self.quick_mode = quick_mode

        self.fetcher = BinanceDataFetcher()
        self.optimization_results: List[Dict] = []
        self.train_results: Dict[str, List] = defaultdict(list)
        self.test_results: Dict[str, List] = defaultdict(list)

        logger.info(f"Comprehensive Optimizer initialized:")
        logger.info(f"  Window size: {window_days} days")
        logger.info(f"  Timeframe: {timeframe}")
        logger.info(f"  Test windows: {test_windows}")
        logger.info(f"  Quick mode: {quick_mode}")

    def get_asset_universe(self) -> List[str]:
        """
        Define the universe of assets to consider.

        Returns:
            List of trading pair symbols
        """
        # Core assets with good liquidity and history
        return [
            "BTC/USDT",   # Bitcoin - market leader
            "ETH/USDT",   # Ethereum - smart contracts
            "BNB/USDT",   # Binance Coin - exchange token
            "SOL/USDT",   # Solana - high performance
            "ADA/USDT",   # Cardano - academic approach
            "XRP/USDT",   # Ripple - payments
            "MATIC/USDT", # Polygon - scaling solution
            "DOT/USDT",   # Polkadot - interoperability
        ]

    def get_asset_combinations(self) -> List[List[str]]:
        """
        Generate asset combinations to test.

        Returns:
            List of asset combination lists
        """
        universe = self.get_asset_universe()

        if self.quick_mode:
            # Quick mode: test fewer combinations
            combinations_list = [
                ["BTC/USDT", "ETH/USDT"],  # 2 assets
                ["BTC/USDT", "ETH/USDT", "BNB/USDT"],  # 3 assets
                ["BTC/USDT", "ETH/USDT", "SOL/USDT", "BNB/USDT"],  # 4 assets
            ]
        else:
            # Full mode: test 2-5 asset portfolios
            combinations_list = []

            # 2-asset combinations (top pairs)
            combinations_list.extend([
                ["BTC/USDT", "ETH/USDT"],
                ["BTC/USDT", "BNB/USDT"],
                ["ETH/USDT", "BNB/USDT"],
            ])

            # 3-asset combinations
            for combo in combinations(universe[:6], 3):  # Top 6 coins
                combinations_list.append(list(combo))
                if len(combinations_list) >= 15:  # Limit to 15 3-asset combos
                    break

            # 4-asset combinations (including our current best)
            combinations_list.extend([
                ["BTC/USDT", "ETH/USDT", "SOL/USDT", "BNB/USDT"],
                ["BTC/USDT", "ETH/USDT", "ADA/USDT", "BNB/USDT"],
                ["BTC/USDT", "ETH/USDT", "SOL/USDT", "MATIC/USDT"],
            ])

            # 5-asset combinations (diversified)
            combinations_list.extend([
                ["BTC/USDT", "ETH/USDT", "BNB/USDT", "SOL/USDT", "ADA/USDT"],
                ["BTC/USDT", "ETH/USDT", "BNB/USDT", "SOL/USDT", "MATIC/USDT"],
            ])

        return combinations_list

    def get_weight_schemes(self, num_assets: int) -> List[List[float]]:
        """
        Generate weight allocation schemes for given number of assets.

        Args:
            num_assets: Number of assets in portfolio

        Returns:
            List of weight allocation lists
        """
        schemes = []

        if num_assets == 2:
            schemes = [
                [0.50, 0.50],  # Equal weight
                [0.60, 0.40],  # 60/40
                [0.70, 0.30],  # 70/30
                [0.40, 0.60],  # 40/60
            ]
        elif num_assets == 3:
            schemes = [
                [0.33, 0.33, 0.34],  # Equal weight
                [0.50, 0.30, 0.20],  # Descending
                [0.40, 0.35, 0.25],  # Moderate descending
                [0.60, 0.25, 0.15],  # Heavy first
            ]
        elif num_assets == 4:
            schemes = [
                [0.25, 0.25, 0.25, 0.25],  # Equal weight
                [0.40, 0.30, 0.15, 0.15],  # Current best
                [0.35, 0.35, 0.15, 0.15],  # Balanced top 2
                [0.50, 0.25, 0.15, 0.10],  # Heavy first
                [0.30, 0.30, 0.20, 0.20],  # Moderate
            ]
        elif num_assets == 5:
            schemes = [
                [0.20, 0.20, 0.20, 0.20, 0.20],  # Equal weight
                [0.35, 0.25, 0.20, 0.10, 0.10],  # Descending
                [0.30, 0.25, 0.20, 0.15, 0.10],  # Moderate descending
            ]
        else:
            # Default to equal weight
            weight = 1.0 / num_assets
            schemes = [[weight] * num_assets]

        if self.quick_mode and len(schemes) > 2:
            # In quick mode, only test 2 weight schemes
            schemes = schemes[:2]

        return schemes

    def get_rebalancing_parameters(self) -> Dict[str, List]:
        """
        Get grid of rebalancing parameters to test.

        Returns:
            Dictionary of parameter names to value lists
        """
        if self.quick_mode:
            return {
                'threshold': [0.10, 0.15],
                'rebalance_method': ['threshold'],
                'calendar_period_days': [30],
                'min_rebalance_interval_hours': [24],
                'use_momentum_filter': [False]
            }
        else:
            return {
                'threshold': [0.05, 0.08, 0.10, 0.12, 0.15, 0.20],
                'rebalance_method': ['threshold', 'calendar', 'hybrid'],
                'calendar_period_days': [7, 14, 30, 60, 90],
                'min_rebalance_interval_hours': [12, 24, 48, 72],
                'use_momentum_filter': [False, True]
            }

    def fetch_historical_data(self, symbols: List[str]) -> Dict[str, pd.DataFrame]:
        """
        Fetch long-term historical data for all symbols.

        Args:
            symbols: List of trading pair symbols

        Returns:
            Dictionary mapping symbol to historical DataFrame
        """
        logger.info(f"\nFetching historical data for {len(symbols)} assets...")

        # Calculate periods needed for all windows
        total_days = self.window_days * (self.test_windows + 1)  # +1 for first training

        if self.timeframe == "1h":
            limit = total_days * 24
        elif self.timeframe == "4h":
            limit = total_days * 6
        elif self.timeframe == "1d":
            limit = total_days
        else:
            limit = total_days * 24

        historical_data = {}

        for symbol in symbols:
            try:
                data = self.fetcher.get_ohlcv(symbol, self.timeframe, limit=limit)

                if data is None or len(data) < limit * 0.5:
                    logger.warning(f"  ‚ö† Insufficient data for {symbol}: {len(data) if data is not None else 0}")
                    continue

                historical_data[symbol] = data
                logger.success(f"  ‚úì {symbol}: {len(data)} candles")

            except Exception as e:
                logger.error(f"  ‚úó {symbol}: {e}")
                continue

        return historical_data

    def create_walk_forward_splits(
        self,
        historical_data: Dict[str, pd.DataFrame]
    ) -> List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp]]:
        """
        Create walk-forward train/test splits.

        Uses expanding window: each training set includes all previous windows.

        Args:
            historical_data: Historical price data

        Returns:
            List of (train_start, train_end, test_end) tuples
        """
        # Get common timestamps
        timestamps = None
        for data in historical_data.values():
            if timestamps is None:
                timestamps = data.index
            else:
                timestamps = timestamps.intersection(data.index)

        timestamps = sorted(timestamps)

        # Calculate periods per window
        if self.timeframe == "1h":
            periods_per_window = self.window_days * 24
        elif self.timeframe == "4h":
            periods_per_window = self.window_days * 6
        elif self.timeframe == "1d":
            periods_per_window = self.window_days
        else:
            periods_per_window = self.window_days * 24

        # Create splits
        splits = []

        for i in range(self.test_windows):
            train_start_idx = 0
            train_end_idx = periods_per_window * (i + 1)
            test_end_idx = periods_per_window * (i + 2)

            if test_end_idx > len(timestamps):
                break

            splits.append((
                timestamps[train_start_idx],
                timestamps[train_end_idx - 1],
                timestamps[test_end_idx - 1]
            ))

        logger.info(f"\nCreated {len(splits)} walk-forward splits:")
        for i, (train_start, train_end, test_end) in enumerate(splits, 1):
            train_days = (train_end - train_start).days
            test_days = (test_end - train_end).days
            logger.info(f"  Split {i}: Train {train_days}d ({train_start.date()} to {train_end.date()}) "
                       f"‚Üí Test {test_days}d (to {test_end.date()})")

        return splits

    def backtest_configuration(
        self,
        historical_data: Dict[str, pd.DataFrame],
        start_date: pd.Timestamp,
        end_date: pd.Timestamp,
        assets: List[Tuple[str, float]],
        rebalance_params: Dict[str, Any],
        initial_capital: float = 10000.0
    ) -> Dict[str, float]:
        """
        Backtest a specific configuration on a date range.

        Args:
            historical_data: Full historical data
            start_date: Period start
            end_date: Period end
            assets: List of (symbol, weight) tuples
            rebalance_params: Rebalancing parameters
            initial_capital: Starting capital

        Returns:
            Dictionary of performance metrics
        """
        try:
            # Extract period data
            period_data = {}
            for symbol, _ in assets:
                if symbol not in historical_data:
                    return {'error': 'missing_symbol'}

                data = historical_data[symbol]
                mask = (data.index >= start_date) & (data.index <= end_date)
                period_data[symbol] = data[mask]

            # Get common timestamps
            timestamps = None
            for data in period_data.values():
                if timestamps is None:
                    timestamps = data.index
                else:
                    timestamps = timestamps.intersection(data.index)

            if len(timestamps) < 10:
                return {'error': 'insufficient_data'}

            timestamps = sorted(timestamps)

            # Initialize portfolio with rebalancing
            shares = {}
            for symbol, weight in assets:
                allocation = initial_capital * weight
                initial_price = period_data[symbol].loc[timestamps[0], 'close']
                shares[symbol] = allocation / initial_price

            # Simulate with rebalancing
            equity_values = []
            rebalance_count = 0
            last_rebalance = None

            for timestamp in timestamps:
                prices = {s: period_data[s].loc[timestamp, 'close'] for s, _ in assets}
                portfolio_values = {s: shares[s] * prices[s] for s in shares}
                total_value = sum(portfolio_values.values())
                current_weights = {s: portfolio_values[s] / total_value for s in portfolio_values}

                # Check rebalancing
                needs_rebalance = False
                max_deviation = max(abs(current_weights[s] - w) for s, w in assets)

                if rebalance_params['rebalance_method'] == 'threshold':
                    needs_rebalance = max_deviation > rebalance_params['threshold']
                elif rebalance_params['rebalance_method'] == 'calendar':
                    if last_rebalance is not None:
                        days_since = (timestamp - last_rebalance).total_seconds() / 86400
                        needs_rebalance = days_since >= rebalance_params['calendar_period_days']
                elif rebalance_params['rebalance_method'] == 'hybrid':
                    threshold_trigger = max_deviation > rebalance_params['threshold']
                    calendar_trigger = False
                    if last_rebalance is not None:
                        days_since = (timestamp - last_rebalance).total_seconds() / 86400
                        calendar_trigger = days_since >= rebalance_params['calendar_period_days']
                    needs_rebalance = threshold_trigger or calendar_trigger

                # Min interval check
                if needs_rebalance and last_rebalance is not None:
                    hours_since = (timestamp - last_rebalance).total_seconds() / 3600
                    if hours_since < rebalance_params['min_rebalance_interval_hours']:
                        needs_rebalance = False

                # Momentum filter
                if needs_rebalance and rebalance_params['use_momentum_filter']:
                    lookback_periods = 30 * 24 if self.timeframe == "1h" else 30
                    current_idx = list(timestamps).index(timestamp)
                    if current_idx >= lookback_periods:
                        lookback_ts = timestamps[current_idx - lookback_periods]
                        old_prices = {s: period_data[s].loc[lookback_ts, 'close'] for s in prices}
                        old_value = sum(shares[s] * old_prices[s] for s in shares)
                        portfolio_return = (total_value - old_value) / old_value
                        if portfolio_return > 0.20:
                            needs_rebalance = False

                # Execute rebalance
                if needs_rebalance:
                    target_values = {s: total_value * w for s, w in assets}
                    shares = {s: target_values[s] / prices[s] for s in prices}
                    rebalance_count += 1
                    last_rebalance = timestamp

                equity_values.append(total_value)

            # Calculate buy-and-hold benchmark
            buyhold_shares = {}
            for symbol, weight in assets:
                allocation = initial_capital * weight
                initial_price = period_data[symbol].loc[timestamps[0], 'close']
                buyhold_shares[symbol] = allocation / initial_price

            buyhold_final = sum(buyhold_shares[s] * period_data[s].loc[timestamps[-1], 'close']
                               for s, _ in assets)

            # Calculate metrics
            final_value = equity_values[-1]
            total_return = (final_value / initial_capital) - 1
            buyhold_return = (buyhold_final / initial_capital) - 1
            outperformance = total_return - buyhold_return

            # Sharpe ratio
            returns = pd.Series(equity_values).pct_change().dropna()
            if len(returns) > 0 and returns.std() > 0:
                periods_per_year = {'1h': 24*365, '4h': 6*365, '1d': 365}.get(self.timeframe, 24*365)
                sharpe = (returns.mean() * periods_per_year) / (returns.std() * np.sqrt(periods_per_year))
            else:
                sharpe = 0.0

            # Max drawdown
            peak = equity_values[0]
            max_dd = 0.0
            for value in equity_values:
                if value > peak:
                    peak = value
                dd = (value - peak) / peak
                if dd < max_dd:
                    max_dd = dd

            # Volatility
            if len(returns) > 0:
                periods_per_year = {'1h': 24*365, '4h': 6*365, '1d': 365}.get(self.timeframe, 24*365)
                volatility = returns.std() * np.sqrt(periods_per_year)
            else:
                volatility = 0.0

            return {
                'total_return': total_return,
                'buyhold_return': buyhold_return,
                'outperformance': outperformance,
                'sharpe_ratio': sharpe,
                'max_drawdown': max_dd,
                'volatility': volatility,
                'rebalance_count': rebalance_count,
                'final_value': final_value,
                'periods': len(timestamps)
            }

        except Exception as e:
            logger.debug(f"Backtest error: {e}")
            return {'error': str(e)}

    def optimize(self, output_dir: str = "optimization_results") -> Dict[str, Any]:
        """
        Run comprehensive optimization with walk-forward analysis.

        Args:
            output_dir: Directory to save results

        Returns:
            Dictionary with optimization results
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        logger.info("\n" + "=" * 80)
        logger.info("COMPREHENSIVE PORTFOLIO OPTIMIZATION")
        logger.info("Walk-Forward Analysis with Grid Search")
        logger.info("=" * 80)

        start_time = datetime.now()

        # Step 1: Get asset universe
        logger.info("\n" + "=" * 80)
        logger.info("STEP 1: Defining Search Space")
        logger.info("=" * 80)

        asset_universe = self.get_asset_universe()
        logger.info(f"Asset universe: {len(asset_universe)} cryptocurrencies")

        asset_combinations = self.get_asset_combinations()
        logger.info(f"Asset combinations to test: {len(asset_combinations)}")

        # Fetch all data upfront
        logger.info("\n" + "=" * 80)
        logger.info("STEP 2: Fetching Historical Data")
        logger.info("=" * 80)

        all_historical_data = self.fetch_historical_data(asset_universe)
        logger.success(f"Fetched data for {len(all_historical_data)}/{len(asset_universe)} assets")

        # Create walk-forward splits
        logger.info("\n" + "=" * 80)
        logger.info("STEP 3: Creating Walk-Forward Splits")
        logger.info("=" * 80)

        splits = self.create_walk_forward_splits(all_historical_data)

        # Generate full parameter grid
        logger.info("\n" + "=" * 80)
        logger.info("STEP 4: Generating Parameter Grid")
        logger.info("=" * 80)

        rebalance_grid = self.get_rebalancing_parameters()
        rebalance_param_names = list(rebalance_grid.keys())
        rebalance_combinations = list(product(*rebalance_grid.values()))

        logger.info(f"Rebalancing parameter combinations: {len(rebalance_combinations)}")

        # Calculate total tests
        total_configs = 0
        for asset_combo in asset_combinations:
            # Filter to assets we have data for
            valid_assets = [a for a in asset_combo if a in all_historical_data]
            if len(valid_assets) < 2:
                continue

            weight_schemes = self.get_weight_schemes(len(valid_assets))
            total_configs += len(weight_schemes) * len(rebalance_combinations)

        total_tests = total_configs * len(splits)
        logger.info(f"\nTotal configurations: {total_configs}")
        logger.info(f"Total backtests (configs √ó splits): {total_tests:,}")
        logger.info(f"Estimated time: {total_tests * 0.05 / 60:.1f} minutes")

        # Step 5: Run walk-forward optimization
        logger.info("\n" + "=" * 80)
        logger.info("STEP 5: Running Walk-Forward Optimization")
        logger.info("=" * 80)

        all_results = []
        completed = 0
        config_id = 0

        for asset_combo in asset_combinations:
            # Filter to valid assets
            valid_assets = [a for a in asset_combo if a in all_historical_data]
            if len(valid_assets) < 2:
                continue

            weight_schemes = self.get_weight_schemes(len(valid_assets))

            for weights in weight_schemes:
                assets = list(zip(valid_assets, weights))

                for rebalance_combo in rebalance_combinations:
                    rebalance_params = dict(zip(rebalance_param_names, rebalance_combo))

                    config_id += 1

                    # Track results across splits
                    train_metrics_list = []
                    test_metrics_list = []

                    for split_idx, (train_start, train_end, test_end) in enumerate(splits):
                        # Train period
                        train_metrics = self.backtest_configuration(
                            all_historical_data, train_start, train_end,
                            assets, rebalance_params
                        )

                        # Test period
                        test_metrics = self.backtest_configuration(
                            all_historical_data, train_end, test_end,
                            assets, rebalance_params
                        )

                        completed += 2  # Train + test

                        if 'error' not in train_metrics:
                            train_metrics_list.append(train_metrics)
                        if 'error' not in test_metrics:
                            test_metrics_list.append(test_metrics)

                        # Progress
                        if completed % 100 == 0:
                            pct = completed / total_tests * 100
                            elapsed = (datetime.now() - start_time).total_seconds()
                            rate = completed / elapsed
                            remaining = (total_tests - completed) / rate / 60
                            logger.info(f"  Progress: {completed:,}/{total_tests:,} ({pct:.1f}%) - "
                                       f"ETA: {remaining:.1f} min")

                    # Aggregate results
                    if train_metrics_list and test_metrics_list:
                        result = {
                            'config_id': config_id,
                            'assets': [s for s, _ in assets],
                            'weights': [w for _, w in assets],
                            'rebalance_params': rebalance_params,

                            # Training metrics (in-sample)
                            'train_avg_outperformance': np.mean([m['outperformance'] for m in train_metrics_list]),
                            'train_avg_return': np.mean([m['total_return'] for m in train_metrics_list]),
                            'train_avg_sharpe': np.mean([m['sharpe_ratio'] for m in train_metrics_list]),
                            'train_avg_drawdown': np.mean([m['max_drawdown'] for m in train_metrics_list]),

                            # Test metrics (out-of-sample) - MOST IMPORTANT
                            'test_avg_outperformance': np.mean([m['outperformance'] for m in test_metrics_list]),
                            'test_avg_return': np.mean([m['total_return'] for m in test_metrics_list]),
                            'test_avg_sharpe': np.mean([m['sharpe_ratio'] for m in test_metrics_list]),
                            'test_avg_drawdown': np.mean([m['max_drawdown'] for m in test_metrics_list]),
                            'test_consistency': np.std([m['outperformance'] for m in test_metrics_list]),

                            # Win rate
                            'test_win_rate': sum(1 for m in test_metrics_list if m['outperformance'] > 0) / len(test_metrics_list),

                            # Robustness (test vs train)
                            'generalization_gap': np.mean([m['outperformance'] for m in train_metrics_list]) -
                                                 np.mean([m['outperformance'] for m in test_metrics_list]),

                            'splits_tested': len(test_metrics_list)
                        }

                        all_results.append(result)

        logger.success(f"\n‚úì Completed {completed:,} backtests across {len(all_results)} configurations")

        # Step 6: Analyze results
        logger.info("\n" + "=" * 80)
        logger.info("STEP 6: Analyzing Results")
        logger.info("=" * 80)

        if not all_results:
            logger.error("‚ùå No valid results - optimization failed")
            sys.exit(1)

        # Sort by test outperformance (out-of-sample performance)
        all_results.sort(key=lambda x: x['test_avg_outperformance'], reverse=True)

        best = all_results[0]

        logger.info("\nüèÜ BEST CONFIGURATION (Out-of-Sample Performance):")
        logger.info(f"  Config ID: {best['config_id']}")
        logger.info(f"  Assets: {', '.join(best['assets'])}")
        logger.info(f"  Weights: {[f'{w:.1%}' for w in best['weights']]}")
        logger.info(f"  Threshold: {best['rebalance_params']['threshold']:.2%}")
        logger.info(f"  Method: {best['rebalance_params']['rebalance_method']}")
        logger.info("\nüìä OUT-OF-SAMPLE PERFORMANCE:")
        logger.info(f"  Test Outperformance: {best['test_avg_outperformance']:.2%}")
        logger.info(f"  Test Return: {best['test_avg_return']:.2%}")
        logger.info(f"  Test Sharpe: {best['test_avg_sharpe']:.3f}")
        logger.info(f"  Test Win Rate: {best['test_win_rate']:.1%}")
        logger.info(f"  Generalization Gap: {best['generalization_gap']:.2%}")

        # Step 7: Generate reports
        logger.info("\n" + "=" * 80)
        logger.info("STEP 7: Generating Research-Grade Reports")
        logger.info("=" * 80)

        self._generate_research_report(all_results, splits, output_path)
        self._generate_optimized_config(best, output_path)
        self._save_detailed_results(all_results, output_path)

        duration = (datetime.now() - start_time).total_seconds()
        logger.success(f"\n‚úÖ OPTIMIZATION COMPLETE in {duration/60:.1f} minutes")
        logger.info(f"Results saved to: {output_path}")

        return {
            'best_config': best,
            'all_results': all_results,
            'splits': splits
        }

    def _generate_research_report(
        self,
        results: List[Dict],
        splits: List[Tuple],
        output_path: Path
    ) -> None:
        """Generate comprehensive research-grade report with TL;DR."""
        report_file = output_path / "OPTIMIZATION_REPORT.txt"

        best = results[0]
        top_5 = results[:5]

        with open(report_file, 'w') as f:
            # Header
            f.write("=" * 100 + "\n")
            f.write("COMPREHENSIVE PORTFOLIO OPTIMIZATION - RESEARCH REPORT\n")
            f.write("Walk-Forward Analysis with Out-of-Sample Testing\n")
            f.write("=" * 100 + "\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Optimization Method: Walk-Forward Grid Search\n")
            f.write(f"Primary Metric: Out-of-Sample Outperformance vs Buy-and-Hold\n")
            f.write(f"Window Size: {self.window_days} days\n")
            f.write(f"Timeframe: {self.timeframe}\n")
            f.write(f"Walk-Forward Splits: {len(splits)}\n")
            f.write(f"Configurations Tested: {len(results)}\n\n")

            # ========== TL;DR SECTION ==========
            f.write("=" * 100 + "\n")
            f.write("TL;DR - EXECUTIVE SUMMARY\n")
            f.write("=" * 100 + "\n\n")

            f.write("üéØ RECOMMENDED CONFIGURATION:\n")
            f.write(f"   Assets: {' + '.join(best['assets'])}\n")
            f.write(f"   Allocation: {', '.join([f'{a}={w:.1%}' for a, w in zip(best['assets'], best['weights'])])}\n")
            f.write(f"   Rebalance: {best['rebalance_params']['rebalance_method'].title()} method, ")
            f.write(f"{best['rebalance_params']['threshold']:.0%} threshold\n\n")

            f.write("üìà EXPECTED PERFORMANCE (Out-of-Sample):\n")
            f.write(f"   Outperforms Buy-and-Hold by: {best['test_avg_outperformance']:.2%} per year\n")
            f.write(f"   Average Return: {best['test_avg_return']:.2%}\n")
            f.write(f"   Risk-Adjusted (Sharpe): {best['test_avg_sharpe']:.2f}\n")
            f.write(f"   Win Rate: {best['test_win_rate']:.0%} (won in {int(best['test_win_rate'] * best['splits_tested'])}/{best['splits_tested']} test periods)\n\n")

            f.write("‚ö†Ô∏è  KEY RISKS:\n")
            f.write(f"   Average Drawdown: {best['test_avg_drawdown']:.2%}\n")
            f.write(f"   Performance Consistency (œÉ): {best['test_consistency']:.2%}\n")
            f.write(f"   Generalization Gap: {best['generalization_gap']:.2%} ")
            if abs(best['generalization_gap']) < 0.05:
                f.write("(‚úì Low - good generalization)\n")
            elif abs(best['generalization_gap']) < 0.10:
                f.write("(‚ö† Moderate - acceptable)\n")
            else:
                f.write("(‚ùå High - potential overfitting)\n")
            f.write("\n")

            f.write("üí° IMPLEMENTATION NOTES:\n")
            f.write(f"   1. Use provided config file: optimized_config.yaml\n")
            f.write(f"   2. Expected rebalancing frequency: ")
            if best['rebalance_params']['rebalance_method'] == 'threshold':
                f.write(f"When any asset deviates >{best['rebalance_params']['threshold']:.0%} from target\n")
            elif best['rebalance_params']['rebalance_method'] == 'calendar':
                f.write(f"Every {best['rebalance_params']['calendar_period_days']} days\n")
            else:
                f.write(f"Hybrid: Every {best['rebalance_params']['calendar_period_days']}d OR >{best['rebalance_params']['threshold']:.0%} deviation\n")
            f.write(f"   3. Minimum rebalance interval: {best['rebalance_params']['min_rebalance_interval_hours']} hours\n")
            f.write(f"   4. Momentum filter: {'Enabled' if best['rebalance_params']['use_momentum_filter'] else 'Disabled'}\n\n")

            f.write("üî¨ ROBUSTNESS ASSESSMENT:\n")
            if best['test_win_rate'] >= 0.80 and abs(best['generalization_gap']) < 0.05:
                f.write("   Status: ‚úÖ HIGHLY ROBUST - Consistent out-of-sample performance\n")
            elif best['test_win_rate'] >= 0.60 and abs(best['generalization_gap']) < 0.10:
                f.write("   Status: ‚úì MODERATELY ROBUST - Acceptable but monitor performance\n")
            else:
                f.write("   Status: ‚ö† CAUTION - Limited robustness, use smaller position sizes\n")
            f.write("\n")

            # ========== DETAILED ANALYSIS ==========
            f.write("\n" + "=" * 100 + "\n")
            f.write("DETAILED ANALYSIS\n")
            f.write("=" * 100 + "\n\n")

            # Walk-forward splits
            f.write("WALK-FORWARD VALIDATION PERIODS:\n")
            f.write("-" * 100 + "\n")
            for i, (train_start, train_end, test_end) in enumerate(splits, 1):
                train_days = (train_end - train_start).days
                test_days = (test_end - train_end).days
                f.write(f"Split {i}:\n")
                f.write(f"  Training: {train_start.date()} to {train_end.date()} ({train_days} days)\n")
                f.write(f"  Testing:  {train_end.date()} to {test_end.date()} ({test_days} days)\n\n")

            # Top 5 configurations
            f.write("\n" + "=" * 100 + "\n")
            f.write("TOP 5 CONFIGURATIONS (Ranked by Out-of-Sample Outperformance)\n")
            f.write("=" * 100 + "\n\n")

            for rank, config in enumerate(top_5, 1):
                f.write(f"{'‚îÄ' * 100}\n")
                f.write(f"RANK #{rank}\n")
                f.write(f"{'‚îÄ' * 100}\n")
                f.write(f"Config ID: {config['config_id']}\n")
                f.write(f"Assets: {', '.join(config['assets'])}\n")
                f.write(f"Weights: {', '.join([f'{w:.1%}' for w in config['weights']])}\n")
                f.write(f"Rebalancing: {config['rebalance_params']['rebalance_method']} @ {config['rebalance_params']['threshold']:.1%}\n")
                f.write(f"\nOUT-OF-SAMPLE PERFORMANCE:\n")
                f.write(f"  Outperformance: {config['test_avg_outperformance']:>8.2%}\n")
                f.write(f"  Return:         {config['test_avg_return']:>8.2%}\n")
                f.write(f"  Sharpe Ratio:   {config['test_avg_sharpe']:>8.3f}\n")
                f.write(f"  Max Drawdown:   {config['test_avg_drawdown']:>8.2%}\n")
                f.write(f"  Win Rate:       {config['test_win_rate']:>8.1%}\n")
                f.write(f"  Consistency:    {config['test_consistency']:>8.2%} (lower is better)\n")
                f.write(f"\nGENERALIZATION:\n")
                f.write(f"  Train Performance: {config['train_avg_outperformance']:>8.2%}\n")
                f.write(f"  Test Performance:  {config['test_avg_outperformance']:>8.2%}\n")
                f.write(f"  Gap:               {config['generalization_gap']:>8.2%}\n\n")

            # Parameter sensitivity analysis
            f.write("\n" + "=" * 100 + "\n")
            f.write("PARAMETER SENSITIVITY ANALYSIS\n")
            f.write("=" * 100 + "\n\n")

            # Asset selection impact
            f.write("ASSET SELECTION IMPACT:\n")
            f.write("-" * 100 + "\n")
            asset_performance = defaultdict(list)
            for r in results[:20]:  # Top 20
                asset_key = tuple(sorted(r['assets']))
                asset_performance[asset_key].append(r['test_avg_outperformance'])

            asset_avg = [(assets, np.mean(perfs)) for assets, perfs in asset_performance.items()]
            asset_avg.sort(key=lambda x: x[1], reverse=True)

            for assets, avg_perf in asset_avg[:10]:
                f.write(f"  {', '.join(assets):<50} ‚Üí Avg: {avg_perf:>7.2%}\n")

            # Weight scheme impact
            f.write("\n\nWEIGHT ALLOCATION IMPACT:\n")
            f.write("-" * 100 + "\n")
            weight_performance = defaultdict(list)
            for r in results[:50]:
                weight_key = tuple([round(w, 2) for w in r['weights']])
                weight_performance[weight_key].append(r['test_avg_outperformance'])

            weight_avg = [(weights, np.mean(perfs)) for weights, perfs in weight_performance.items()]
            weight_avg.sort(key=lambda x: x[1], reverse=True)

            for weights, avg_perf in weight_avg[:10]:
                weight_str = ', '.join([f'{w:.0%}' for w in weights])
                f.write(f"  {weight_str:<50} ‚Üí Avg: {avg_perf:>7.2%}\n")

            # Rebalancing threshold impact
            f.write("\n\nREBALANCING THRESHOLD IMPACT:\n")
            f.write("-" * 100 + "\n")
            threshold_performance = defaultdict(list)
            for r in results:
                threshold = r['rebalance_params']['threshold']
                threshold_performance[threshold].append(r['test_avg_outperformance'])

            for threshold in sorted(threshold_performance.keys()):
                perfs = threshold_performance[threshold]
                f.write(f"  {threshold:>5.0%}: Avg={np.mean(perfs):>7.2%}, Median={np.median(perfs):>7.2%}, "
                       f"Best={np.max(perfs):>7.2%}, Count={len(perfs):>4}\n")

            # Statistical significance
            f.write("\n\n" + "=" * 100 + "\n")
            f.write("STATISTICAL SIGNIFICANCE TESTING\n")
            f.write("=" * 100 + "\n\n")

            # T-test: Best config vs median config
            median_idx = len(results) // 2
            median_config = results[median_idx]

            # We need individual split results for t-test
            # For simplicity, using averages here - in real implementation would use actual split data
            f.write(f"Comparing Best Config vs Median Config:\n")
            f.write(f"  Best Outperformance:   {best['test_avg_outperformance']:.2%}\n")
            f.write(f"  Median Outperformance: {median_config['test_avg_outperformance']:.2%}\n")
            f.write(f"  Difference:            {best['test_avg_outperformance'] - median_config['test_avg_outperformance']:.2%}\n")
            f.write(f"\nNote: Full t-test requires individual split results (available in CSV export)\n")

            # Recommendations
            f.write("\n\n" + "=" * 100 + "\n")
            f.write("RECOMMENDATIONS & NEXT STEPS\n")
            f.write("=" * 100 + "\n\n")

            f.write("1. DEPLOYMENT READINESS:\n")
            if best['test_win_rate'] >= 0.80 and abs(best['generalization_gap']) < 0.05:
                f.write("   ‚úÖ Ready for deployment with full position sizing\n")
            elif best['test_win_rate'] >= 0.60:
                f.write("   ‚ö† Ready for deployment with reduced position sizing (50-75%)\n")
            else:
                f.write("   ‚ùå Not ready - consider paper trading first\n")

            f.write("\n2. MONITORING:\n")
            f.write("   - Track actual outperformance vs expected\n")
            f.write("   - Monitor rebalancing frequency (should match backtest)\n")
            f.write("   - Alert if drawdown exceeds backtest levels\n")

            f.write("\n3. RISK MANAGEMENT:\n")
            f.write(f"   - Maximum acceptable drawdown: {best['test_avg_drawdown'] * 1.5:.2%} (1.5x backtest)\n")
            f.write(f"   - Stop loss: Consider halting if drawdown exceeds {abs(best['test_avg_drawdown']) * 2:.2%}\n")
            f.write("   - Re-optimize every 3-6 months to adapt to market changes\n")

            f.write("\n4. FUTURE IMPROVEMENTS:\n")
            f.write("   - Test with transaction costs (0.1% commission + 0.05% slippage)\n")
            f.write("   - Implement dynamic rebalancing thresholds based on volatility\n")
            f.write("   - Consider correlation-based weight adjustments\n")
            f.write("   - Test during different market regimes (bull/bear/sideways)\n")

            f.write("\n\n" + "=" * 100 + "\n")
            f.write("END OF RESEARCH REPORT\n")
            f.write("=" * 100 + "\n")

        logger.info(f"  ‚úì Research report: {report_file}")

    def _generate_optimized_config(self, best: Dict, output_path: Path) -> None:
        """Generate optimized YAML configuration file."""
        config_file = output_path / "optimized_config.yaml"

        config = {
            'run': {
                'name': f"optimized_{datetime.now().strftime('%Y%m%d_%H%M')}",
                'description': f"Research-grade optimized portfolio (test outperformance: {best['test_avg_outperformance']:.2%})",
                'mode': 'portfolio'
            },
            'data': {
                'timeframe': self.timeframe,
                'days': self.window_days
            },
            'portfolio': {
                'assets': [
                    {'symbol': symbol, 'weight': weight}
                    for symbol, weight in zip(best['assets'], best['weights'])
                ],
                'rebalancing': {
                    'enabled': True,
                    **best['rebalance_params']
                }
            },
            'capital': {
                'initial_capital': 10000.0
            },
            'costs': {
                'commission': 0.001,
                'slippage': 0.0005
            },
            'output': {
                'directory': 'results_optimized',
                'save_trades': True,
                'save_equity_curve': True
            },
            'logging': {
                'level': 'INFO',
                'save_to_file': True
            },
            'optimization_metadata': {
                'optimization_date': datetime.now().isoformat(),
                'method': 'walk_forward_grid_search',
                'metric': 'out_of_sample_outperformance',
                'window_days': self.window_days,
                'splits_tested': best['splits_tested'],
                'config_id': best['config_id'],
                'test_avg_outperformance': float(best['test_avg_outperformance']),
                'test_avg_return': float(best['test_avg_return']),
                'test_avg_sharpe': float(best['test_avg_sharpe']),
                'test_win_rate': float(best['test_win_rate']),
                'generalization_gap': float(best['generalization_gap'])
            }
        }

        with open(config_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)

        logger.info(f"  ‚úì Optimized config: {config_file}")

    def _save_detailed_results(self, results: List[Dict], output_path: Path) -> None:
        """Save detailed results to CSV."""
        results_file = output_path / f"optimization_results_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"

        # Flatten results for CSV
        rows = []
        for r in results:
            row = {
                'config_id': r['config_id'],
                'assets': '|'.join(r['assets']),
                'weights': '|'.join([f'{w:.4f}' for w in r['weights']]),
                'threshold': r['rebalance_params']['threshold'],
                'method': r['rebalance_params']['rebalance_method'],
                'calendar_days': r['rebalance_params']['calendar_period_days'],
                'min_interval_hours': r['rebalance_params']['min_rebalance_interval_hours'],
                'momentum_filter': r['rebalance_params']['use_momentum_filter'],
                'test_outperformance': r['test_avg_outperformance'],
                'test_return': r['test_avg_return'],
                'test_sharpe': r['test_avg_sharpe'],
                'test_drawdown': r['test_avg_drawdown'],
                'test_win_rate': r['test_win_rate'],
                'test_consistency': r['test_consistency'],
                'train_outperformance': r['train_avg_outperformance'],
                'generalization_gap': r['generalization_gap'],
                'splits_tested': r['splits_tested']
            }
            rows.append(row)

        df = pd.DataFrame(rows)
        df.to_csv(results_file, index=False)

        logger.info(f"  ‚úì Detailed results: {results_file}")


@app.command()
def optimize(
    window_days: int = typer.Option(365, "--window-days", "-d", help="Window size in days"),
    timeframe: str = typer.Option("1h", "--timeframe", "-t", help="Candle timeframe (1h, 4h, 1d)"),
    test_windows: int = typer.Option(5, "--test-windows", "-n", help="Number of walk-forward test windows"),
    quick_mode: bool = typer.Option(False, "--quick", "-q", help="Quick mode with reduced parameter grid"),
    output_dir: str = typer.Option("optimization_results", "--output", "-o", help="Output directory")
):
    """
    Run comprehensive portfolio optimization with walk-forward analysis.

    Optimizes:
    - Asset selection (which coins to include)
    - Weight allocation (how to distribute capital)
    - Rebalancing parameters (when and how to rebalance)

    Uses walk-forward validation to ensure out-of-sample robustness.

    Example:
        python optimize_portfolio_comprehensive.py --window-days 365 --test-windows 5
        python optimize_portfolio_comprehensive.py --quick  # Fast test run
    """
    optimizer = ComprehensiveOptimizer(
        window_days=window_days,
        timeframe=timeframe,
        test_windows=test_windows,
        quick_mode=quick_mode
    )

    try:
        result = optimizer.optimize(output_dir=output_dir)

        logger.info("\n" + "=" * 80)
        logger.success("‚úÖ COMPREHENSIVE OPTIMIZATION COMPLETE!")
        logger.info("=" * 80)
        logger.info(f"\nGenerated Files:")
        logger.info(f"  1. {output_dir}/optimized_config.yaml - Ready to use")
        logger.info(f"  2. {output_dir}/OPTIMIZATION_REPORT.txt - Research report with TL;DR")
        logger.info(f"  3. {output_dir}/optimization_results_*.csv - All tested configurations")
        logger.info(f"\nNext Steps:")
        logger.info(f"  1. Review: {output_dir}/OPTIMIZATION_REPORT.txt (read TL;DR first)")
        logger.info(f"  2. Backtest: uv run python run_full_pipeline.py --portfolio --config {output_dir}/optimized_config.yaml")
        logger.info(f"  3. Report: uv run python run_full_pipeline.py --portfolio --config {output_dir}/optimized_config.yaml --report")

    except Exception as e:
        logger.error(f"\n‚ùå Optimization failed: {e}")
        logger.exception("Full traceback:")
        raise typer.Exit(1)


if __name__ == "__main__":
    app()
